{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04936d0-238f-4f14-8a67-c52d54a27507",
   "metadata": {},
   "source": [
    "# Intuitions on  ensemble tree-based models\n",
    "\n",
    "## Bagging vs random forests\n",
    "\n",
    "Bagging is a general strategy\n",
    "* Can work with any base model (linear, trees, ...)\n",
    "\n",
    "Random forests are bagged *randomized* decision trees\n",
    "* At each split: a random subset of features are selected\n",
    "* The best split is taken among the restricted subset\n",
    "* Extra randomization decorrelates the prediction errors\n",
    "* Uncorrelated errors make bagging work better\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "<ul>\n",
    "    <li>It's fine to use deep trees (max_depth=None) in random forests because of the reduced overfitting effect of prediction averaging. </li>\n",
    "    <li>The more trees the better, typical to use 100 trees or more.</li>\n",
    "    <li>More trees: longer to fit, slower to predict and bigger models to deploy.</li></ul>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be183d-961b-41b1-b529-56d6b5b5835e",
   "metadata": {},
   "source": [
    "In practice, gradient boosting is more flexible thanks to the use of cost functions and tend to exhibits better predictive performance than traditional boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5516cc-f4bf-4a8c-a248-fe97e6f5a8ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Take away:</b> <br>\n",
    "    <ul>\n",
    "        <li> Bagging and random forests fit trees independently\n",
    "        <ul>    \n",
    "        <li> each deep tree overfits individually </li>\n",
    "            <li> averaging the tree predictions reduces overfitting</li></ul></li>\n",
    "        <li> (Gradient) boosting fits trees sequentially\n",
    "            <ul>\n",
    "            <li> each shallow tree underfits individually</li>\n",
    "                <li> sequentially adding trees reduces underfitting</li></ul></li>\n",
    "        <li> Gradient boosting tends to perform slightly better than bagging and random forest and furthermore shallow trees predict faster</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf986a-6654-4a19-b035-7797ea84a135",
   "metadata": {},
   "source": [
    "## Introductory example to ensemble models\n",
    "\n",
    "This notebook aims at emphasizing the benefit of ensemble methods over simple models (e.g. decision tree, linear model, etc.). Combining simple models result in more powerful and robust models with less hassle.\n",
    "\n",
    "We will be working with the California housing dataset. We recall that the goal in this dataset is to predict the median house value in some district in California based on demographic and geographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6eca60-c753-418e-8c13-f49734e883b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "target += 100 # rescale the target in k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde9cec-f05e-42a2-9b82-1d8b8eeb164f",
   "metadata": {},
   "source": [
    "We will check the generalization performance of decision tree regressor with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0291766c-1d23-458f-9194-41a3287c83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "cv_results = cross_validate(tree, data, target, n_jobs=2)\n",
    "scores = cv_results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5d1132-9fa2-4e91-a069-f43c313fff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score obtained by cross-validation: 0.345 +/- 0.104\n"
     ]
    }
   ],
   "source": [
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "     f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96342c39-b59c-476e-847a-bbb5ed37ea84",
   "metadata": {},
   "source": [
    "We obtain fair results. However, as we previously presented in the \"tree in depth\" notebook, this model needs to be tuned to overcome over- or under-fitting. Indeed, the default parameters will not necessarily lead to an optimal decision tree. Instead of using the default value, we sould search via cross-validation the optimal value of the important parameters such as `max_depth`, `min_samples_split`, or `min_samples_leaf`.\n",
    "\n",
    "We recall that we need to tune these parameters, as decision trees tend to overfit the training data if we grow deep trees, but there are no rules on what each parameter should be set to. Thus, not making a search could lead us to have an underfitted or overfitted model.\n",
    "\n",
    "Now, we make a grid-search to tune the hyperparameters that we nemtioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19719d42-722b-459c-a4aa-ce3cf3a43a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score obtained by cross-validation: 0.523 +/- 0.108\n",
      "CPU times: user 38.5 ms, sys: 42.3 ms, total: 80.8 ms\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [5, 8, None],\n",
    "    \"min_samples_split\": [2, 10, 30, 50],\n",
    "    \"min_samples_leaf\": [0.01, 0.05, 0.1, 1]}\n",
    "cv = 3\n",
    "\n",
    "tree = GridSearchCV(DecisionTreeRegressor(random_state=0),\n",
    "                    param_grid=param_grid, cv=cv, n_jobs=2)\n",
    "cv_results = cross_validate(tree, data, target, n_jobs=2,\n",
    "                            return_estimator=True)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c76a6b3-f934-4ea2-af51-bb8daccaf050",
   "metadata": {},
   "source": [
    "As expected, optimizing the hyperparameters has a positive effect on the generalization performance. However, it comes with a higher computational cost.\n",
    "\n",
    "We can create a dataframe storing the important information collected during the tuning of the parameters and investigate the results.\n",
    "\n",
    "Now we will use an ensemble method called bagging. In short, this method will use a base regressor (i.e. decision tree regressors) and will train several of them on a slightly modified version of the training set. Then the predictions of all these base regressors will be combined by averaging.\n",
    "\n",
    "There, we will use 20 decision trees and check the fitting time as well as the generalization performance on the left-out testing data. It is important to note that we are not going to tune any parameter of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc1513a7-5b1d-4f01-89bd-e583b21b75e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score obtained by cross-validation: 0.639 +/- 0.083\n",
      "CPU times: user 20.9 ms, sys: 8.64 ms, total: 29.6 ms\n",
      "Wall time: 7.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "base_estimator = DecisionTreeRegressor(random_state=0)\n",
    "bagging_regressor = BaggingRegressor(base_estimator=base_estimator,\n",
    "                                    n_estimators=20, random_state=0)\n",
    "cv_results = cross_validate(bagging_regressor, data, target, n_jobs=2)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "     f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f31f9-4ff0-4789-8823-6cdaf8f2122f",
   "metadata": {},
   "source": [
    "Without searching for optimal hyperparameters, the overall generalization performance of the bagging regressor is better than a single decision tree. In addition, the computational cost is reduced in comparison to optimal hyperparameters.\n",
    "\n",
    "This shows the motivation behind the use of an ensemble learner: it gives a relatively good baseline with decent generalization performance without any parameter tuning.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
