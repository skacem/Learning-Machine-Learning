{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9dd9ee1-7c35-48ac-8dc4-1edb531e124c",
   "metadata": {},
   "source": [
    "# Exercise 302\n",
    "\n",
    "The goal is to find the best set of hyperparameters which maximize the generalization performance on training set.\n",
    "\n",
    "Here again we limit the size of the training set to make computation run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d3a698-fe36-4db7-bafc-d690269de780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af25277f-6484-4faf-932c-969ee5d2f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/adult-census.csv')\n",
    "target = df['class']\n",
    "data = df.drop(columns=['class', 'fnlwgt', 'education-num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb749aa9-a9e9-4d58-a3b8-6c48d1d771eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d375a4ba-583a-4fa9-8efb-bd48b3a3bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(data, target,\n",
    "                                                                   train_size=.2,\n",
    "                                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a788dfa0-96d0-4199-a75b-d32ef0b1397d",
   "metadata": {},
   "source": [
    "In this exercise, we will progressively define the classification pipeline and later tune its hyperparameters.\n",
    "\n",
    "Our pipeline, should:  \n",
    "\n",
    "* preprocess the categorical columns using a `OneHotEncoder` and use a `StandardScaler` to normalize the numerical data.\n",
    "* use a `LogisticRegression` as a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1cc795-4bf0-47f0-a2ac-9cf41702f25e",
   "metadata": {},
   "source": [
    "We start by defining the columns and the preprocessing pipelines to be applied on each group of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b573e2-941a-4827-b1d7-fc191bd68553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "categorical_columns_selector = selector(dtype_include='object')\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude='object')\n",
    "numerical_columns = numerical_columns_selector(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb34d82-769a-4f11-a774-cf3ee660a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "categorical_pocessor = OneHotEncoder(handle_unknown='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
