{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c21b604-eb7e-4492-9622-da98b592e929",
   "metadata": {},
   "source": [
    "# Exercise 106\n",
    "\n",
    "The goal of this exercise is to evaluate the impact of feature preprocessing on a pipeline that uses a decision-tree-based classifier instead of a logistic regression.\n",
    "\n",
    "* The first question is to empirically evaluate whether scaling numerical features is helpful or not;\n",
    "* The second question is to evaluate whether it is empirically better (both from a computational and a statistical perspective) to use integer coded or one-hot encoded categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d24be3-7c45-4205-936d-7eef2e1cc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Disable jedi autocompleter\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5148f98-0d4f-4f71-a642-3017aaeeb98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/adult-census.csv')\n",
    "\n",
    "target = df['class']\n",
    "data = df.drop(columns=['class', 'education-num', 'fnlwgt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ee975-1132-441c-b55a-82b3bfa487d1",
   "metadata": {},
   "source": [
    "As in the previous notebook, we use the utility `make_column_selector` to select only columns with a specific data type. Besides, we list in advance all categories for the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97123418-6dda-41e5-b11e-55111e64d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71cb10f9-e92e-4020-953f-f2e621cd6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d28a22-204f-49b4-92f2-d43e46bf95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2130e-7f89-4dbc-8312-207297714aa1",
   "metadata": {},
   "source": [
    "## Reference Pipeline (no numerical scaling and integer-coded categories)\n",
    "\n",
    "First let's time the pipeline used in the main notebook to serve as a reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fac43f-1f39-4019-a6ee-648fe5bb2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "126b9fdd-2305-4680-b32f-635d248de44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross-validation accuracy is: 0.874 +/- 0.003 with a fitting time of 8.348\n"
     ]
    }
   ],
   "source": [
    "categorical_prep = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=-1)\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_prep, categorical_columns)],\n",
    "    remainder='passthrough')\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())\n",
    "\n",
    "start = time.time()\n",
    "res = cross_validate(model, data, target)\n",
    "elapsed_time = time.time() - start\n",
    "\n",
    "scores = res['test_score']\n",
    "\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0e93d-d2c6-4890-8be3-4a163ae1d02d",
   "metadata": {},
   "source": [
    "## Scaling Numerical Features\n",
    "\n",
    "Let's write a similar pipeline that also scales the numerical features using `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9442355d-3536-4345-aecc-d6f487595ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d114e974-8e34-440c-8698-dea95b54fdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross-validation accuracy is: 0.873 +/- 0.003 with a fitting time of 8.972\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', StandardScaler(), numerical_columns),\n",
    "    ('categorical', categorical_prep, categorical_columns)])\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())\n",
    "\n",
    "start = time.time()\n",
    "res = cross_validate(model, data, target)\n",
    "elapsed_time = time.time() - start\n",
    "\n",
    "scores = res['test_score']\n",
    "\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b434d1d8-cdf9-4b7b-8449-496017536518",
   "metadata": {},
   "source": [
    "As we can see from the above examples, both accuracy and training time are approximately the same as the reference pipeline.\n",
    "\n",
    "Scaling numerical features in indeed useless for most decision tree models in general and for `HistGradientBoostingClassifier` in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19922f4-3cc5-4681-98bc-d05f5058e67e",
   "metadata": {},
   "source": [
    "## One-hot Encoding of Categorical Variables\n",
    "\n",
    "We observed that integer coding of categorical variables can be very detrimental for linear models. However, it does not seem to be the case for `HistGradientBoostingClassifier` models, as the cross-validation score of the reference pipeline with `OrdinalEncoder` is reasonably good.\n",
    "\n",
    "Let's see if we can get an even better accuracy with `OneHotEncoder`.\n",
    "\n",
    "Hint: `HistGradientBoostingClassifier` does not yet support sparse input data. You might want to use `OneHotEncoder(handle_unknown=\"ignore\", sparse=False)` to force the use of a dense representation as a workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ec6dc7f-8f96-4d49-85c4-811d2423ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4af1ffb6-abf7-440e-a35f-920c062d73d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross-validation accuracy is: 0.873 +/- 0.003 with a fitting time of 24.555\n"
     ]
    }
   ],
   "source": [
    "categorical_prep = OneHotEncoder(handle_unknown=\"ignore\",\n",
    "                                  sparse=False)\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_prep, categorical_columns)],\n",
    "    remainder='passthrough')\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())\n",
    "\n",
    "start = time.time()\n",
    "res = cross_validate(model, data, target)\n",
    "elapsed_time = time.time() - start\n",
    "\n",
    "scores = res['test_score']\n",
    "\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863e311-48c3-41aa-9c37-c2fe455db3a2",
   "metadata": {},
   "source": [
    "From an accuracy point of view, the result is almost exactly the same. The reason is that `HistGradientBoostingClassifier` is expressive and robust enough to deal with misleading ordering of integer coded categories (which was not the case for linear models).\n",
    "\n",
    "However from a computation point of view, the training time is significantly longer: this is caused by the fact that `OneHotEncoder` generates approximately 10 times more features than `OrdinalEncoder`.\n",
    "\n",
    "Note that the current implementation `HistGradientBoostingClassifier` is still incomplete, and once sparse representation are handled correctly, training time might improve with such kinds of encodings.\n",
    "\n",
    "The main take away message is that arbitrary integer coding of categories is perfectly fine for `HistGradientBoostingClassifier` and yields fast training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b07c79-072f-44e1-afd4-5a3505d5d9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
