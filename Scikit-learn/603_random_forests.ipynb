{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e17d205-b12c-4ff7-aa2e-ee86dc1fc481",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "In this notebook, we will present the random forest models and show the differences with the bagging ensembles.\n",
    "\n",
    "Random forests are a popular model in machine learning. They are a modification of the bagging algorithm. In bagging, any classifier or regressor can be used. In random forests, the base classifier or regressor is always a decision tree.\n",
    "\n",
    "Random forests have another particularity: when training a tree, the search for the best split is done only on a subset of the original features taken at random. The random subsets are different for each split node. The goal is to inject additional randomization into the learning procedure to try to decorrelate the prediction errors of the individual trees.\n",
    "\n",
    "Therefore, random forests are using randomization on both axes of the data matrix:\n",
    "* by bootstrapping samples for each tree in the forest;\n",
    "* randomly selecting a subset of features at each node of the tree.\n",
    "\n",
    "## A look at random forests\n",
    "\n",
    "We will illustrate the usage of a random forest classifier on the adult census dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ced2a2-2777-44e1-9da4-c83d150e31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adult_census = pd.read_csv('data/adult-census.csv')\n",
    "target_name = 'class'\n",
    "data = adult_census.drop(columns=[target_name, 'education-num'])\n",
    "target = adult_census[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfe1c7-c71e-4c50-84c4-8d7b949f7058",
   "metadata": {},
   "source": [
    "The adult census contains some categorical data and we encode the categorical features using an `OrdinalEncoder` since tree-based models can work very efficiently with such a naive representation of categorical variables.\n",
    "\n",
    "Since there are rare categories in this dataset we need to specifically encode unknown categories at prediction time in order to be able to use cross-validation. Otherwise some rare categories could only be present on the validation side of the cross-validation split and the `OrdinalEncoder` would raise a n error when calling the `transform` method with the data points of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0244c42-da90-4134-9f5d-5de9e65fd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "categorical_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', \n",
    "                                    unknown_value=-1)\n",
    "preprocessor = make_column_transformer(\n",
    "        (categorical_encoder, make_column_selector(dtype_include=object)),\n",
    "        remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365fa827-2855-4d27-b61a-3740feae1af4",
   "metadata": {},
   "source": [
    "We will first give a simple example where we will train a single decision tree classifier and check its generalization performance via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb8cbf4-ba8d-4313-8459-7f509400c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = make_pipeline(preprocessor, DecisionTreeClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64648cb1-f213-4f2d-adfc-7adb7e469541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree classifier: 0.812 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_tree = cross_val_score(tree, data, target)\n",
    "print(f\"Decision tree classifier: \"\n",
    "     f\"{scores_tree.mean():.3f} +/- {scores_tree.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d31b97-230b-406b-8684-8d4ff2e5fe5c",
   "metadata": {},
   "source": [
    "Similarly to what was done in the previous notebook, we construct a `BaggingClassifier` with a decision tree classifier as base model. In addition, we need to specify how many models do we want to combine. Note that we also need to preprocess the data and thus use a scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef0ac496-900e-41f8-89d2-32416ca0efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagged_trees = make_pipeline(preprocessor,\n",
    "                            BaggingClassifier(\n",
    "                                base_estimator=DecisionTreeClassifier(random_state=0),\n",
    "                            n_estimators=50, n_jobs=2, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d01bae-259b-4b17-bc4a-6118946ee2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged decision tree classifier: 0.853 +/- 0.003\n"
     ]
    }
   ],
   "source": [
    "scores_bagged_trees = cross_val_score(bagged_trees, data, target)\n",
    "\n",
    "print(f\"Bagged decision tree classifier: \"\n",
    "      f\"{scores_bagged_trees.mean():.3f} +/- {scores_bagged_trees.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f3167-b745-4bdf-8304-9e37ac2754cd",
   "metadata": {},
   "source": [
    "Note that the generalization performance of the bagged trees is already much better than the performance of a single tree.\n",
    "\n",
    "Now we will use a random forest. You will observe that we do not need to specify any `base_estimator` because the estimator is forced to be a decision tree. Thus, we just specify the desired number of trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c4d997e-a011-43fe-90c3-d6b9ba96c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(n_estimators=50, n_jobs=2, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8f5a793-7361-4708-b5bc-f21b72f2f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier: 0.856 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "scores_random_forest = cross_val_score(random_forest, data, target)\n",
    "\n",
    "print(f\"Random forest classifier: \"\n",
    "      f\"{scores_random_forest.mean():.3f} +/- \"\n",
    "      f\"{scores_random_forest.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855ede9-c657-40b6-8dfc-263aa480008a",
   "metadata": {},
   "source": [
    "It seems that the random forest is performing slightly better than the bagged trees possibly due to the randomized selection of the features which decorrelates the prediction errors of individual trees and as a consequence make the averaging step more efficient at reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233690b-a1ba-4eb5-8b99-cdfbc4fea321",
   "metadata": {},
   "source": [
    "## Details about default hyperparameters\n",
    "\n",
    "For random forests, it is possible to control the amount of randomness for each split by setting the value of `max_features` hyperparameters:\n",
    "\n",
    "* max_features=0.5 means that 50\\% of the features are considered at each split;\n",
    "* max_features=1.0 means that all features are considered at each split which effectively disables features subsampling.\n",
    "\n",
    "By default, `RandomForestRegressor` disables features subsampling while `RandomforestClassifer` uses `max_features=np.sqrt(n_features)`. These default values reflect good practices given in the scientific literature.\n",
    "\n",
    "However, `max_features` is one of the hyperparameters to consider when tuning a random forest:\n",
    "\n",
    "* too much randomness in the trees can lead to underfitted base models and can be detrimental for the ensemble as a whole,\n",
    "* too few randomness in the trees leads to more correlation of the prediction errors and as a result reduce the benefits of the averaging step in therms of overfitting control.\n",
    "\n",
    "The bagging classes also expose a `max_features` parameter. However, `BaggingClassifier` and `BaggingRegressor` are agnostic with respect to their vase model and therefore random feature subsampling can only happpen once before fitting each base model instead of several times per base model as is the case when adding splits to a given tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9a667-30dc-4e69-b724-270c988c3865",
   "metadata": {},
   "source": [
    "We summarize these details in the following table:\n",
    "\n",
    "| Ensemble model class     | Base model class          | Default value for `max_features`   | Features subsampling strategy |\n",
    "|--------------------------|---------------------------|------------------------------------|-------------------------------|\n",
    "| `BaggingClassifier`      | User specified (flexible) | `n_features` (no subsampling) | Model level                   |\n",
    "| `RandomForestClassifier` | `DecisionTreeClassifier`  | `sqrt(n_features)`                 | Tree node level               |\n",
    "| `BaggingRegressor`       | User specified (flexible) | `n_features` (no subsampling) | Model level                   |\n",
    "| `RandomForestRegressor`  | `DecisionTreeRegressor`   | `n_features` (no subsampling) | Tree node level               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a3667-c09c-471e-8840-a022eaae4296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
